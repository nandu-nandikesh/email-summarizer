{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279956ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nandi\\OneDrive\\Desktop\\traffic\\email summ\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8627dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load email-specific pretrained model\n",
    "model_name = \"IrisWiris/email-summarizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"✅ Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd479e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BEFORE FINE-TUNING:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Length: 234 words\n",
      "Summary: Sarah is pleased to provide a comprehensive update on the Q1 marketing campaign project for the past two months. She has conducted extensive market analysis, competitor research, and customer surveys, and has prepared a detailed breakdown to share in tomorrow's meeting. The initial budget of $50,000 may need to be increased by 15% due to rising advertising costs on social media platforms. Sarah will present findings along with recommendations next week.\n",
      "Summary Length: 71 words\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the summarizer pipeline with your base model\n",
    "summarizer = pipeline(\"summarization\", model=\"IrisWiris/email-summarizer\")  # or whichever model you used for training\n",
    "\n",
    "# Sample long email\n",
    "sample_email = \"\"\"\n",
    "Subject: Project Update and Next Steps\n",
    "\n",
    "Hi Team,\n",
    "\n",
    "I hope this email finds you well. I wanted to provide a comprehensive update on the Q1 marketing campaign project that we've been working on for the past two months.\n",
    "\n",
    "First, I'm pleased to announce that we've successfully completed the initial research phase. Our team conducted extensive market analysis, competitor research, and customer surveys. The data shows promising opportunities in the millennial demographic, particularly in urban areas.\n",
    "\n",
    "Second, regarding the budget allocation, we need to discuss some adjustments. The initial budget of $50,000 may need to be increased by 15% due to rising advertising costs on social media platforms. I've prepared a detailed breakdown that I'll share in tomorrow's meeting.\n",
    "\n",
    "Third, the creative team has developed three campaign concepts. Each concept has been tested with focus groups, and we have clear feedback on which direction resonates most with our target audience. I'll be presenting these findings along with my recommendations next week.\n",
    "\n",
    "Finally, I want to address the timeline concerns raised in last week's meeting. While we're slightly behind schedule due to the extended research phase, I'm confident we can still launch by the end of Q2 if we expedite the design phase and bring in additional resources.\n",
    "\n",
    "Please review the attached documents before our meeting on Friday at 2 PM. If you have any questions or concerns, don't hesitate to reach out.\n",
    "\n",
    "Best regards,\n",
    "Sarah\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary with pretrained model\n",
    "print(\"=\"*50)\n",
    "print(\"BEFORE FINE-TUNING:\")\n",
    "print(\"=\"*50)\n",
    "summary_before = summarizer(sample_email, max_length=100, min_length=30, do_sample=False)\n",
    "print(f\"\\nOriginal Length: {len(sample_email.split())} words\")\n",
    "print(f\"Summary: {summary_before[0]['summary_text']}\")\n",
    "print(f\"Summary Length: {len(summary_before[0]['summary_text'].split())} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b15d3b",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c7b256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8 emails\n",
      "\n",
      "Sample data:\n",
      "                                               email  \\\n",
      "0  Subject: Meeting Reminder\\n\\nHi team, just a q...   \n",
      "1  Subject: Vacation Request\\n\\nDear Manager, I w...   \n",
      "2  Subject: Bug Report\\n\\nHello Support Team, I'm...   \n",
      "3  Subject: Project Update\\n\\nHi Team, I wanted t...   \n",
      "4  Subject: Server Maintenance Notice\\n\\nDear All...   \n",
      "\n",
      "                                             summary  \n",
      "0  Weekly sync meeting reminder for tomorrow at 1...  \n",
      "1  Vacation leave request from June 1-15 with cov...  \n",
      "2  Critical login authentication bug affecting 20...  \n",
      "3  Q1 marketing campaign update: research complet...  \n",
      "4  Server maintenance scheduled next weekend, mul...  \n"
     ]
    }
   ],
   "source": [
    "# Load your email data\n",
    "# Format: CSV with columns 'email' and 'summary'\n",
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\nandi\\OneDrive\\Desktop\\traffic\\email summ\\data\\emails.csv\")\n",
    "except FileNotFoundError:\n",
    "    # Create sample dataset if CSV not found\n",
    "    sample_data = {\n",
    "        'email': [\n",
    "             \"Subject: Meeting Reminder\\n\\nHi team, just a quick reminder that we have our weekly sync meeting tomorrow at 10 AM. Please come prepared with your updates and any blockers you're facing. Looking forward to seeing everyone there.\",\n",
    "            \"Subject: Vacation Request\\n\\nDear Manager, I would like to request vacation leave from June 1st to June 15th for a family trip. I have completed all my pending tasks and briefed John to cover for me during my absence. Please let me know if this works.\",\n",
    "            \"Subject: Bug Report\\n\\nHello Support Team, I'm experiencing a critical issue with the login module. Users are unable to authenticate using their credentials since this morning. Error code 500 is being displayed. This is affecting approximately 200 users. Please prioritize this issue.\"\n",
    "        ],\n",
    "        'summary': [\n",
    "            \"Weekly sync meeting reminder for tomorrow at 10 AM.\",\n",
    "            \"Vacation leave request from June 1-15 with coverage arranged.\",\n",
    "            \"Critical login authentication bug affecting 200 users, needs immediate attention.\"\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"Dataset size: {len(df)} emails\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f7590",
   "metadata": {},
   "source": [
    "Data Conversion and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26552720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 10.25 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 28.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data preprocessing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize emails (inputs)\n",
    "    model_inputs = tokenizer(\n",
    "        examples['email'], \n",
    "        max_length=512, \n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    # Tokenize summaries (labels)\n",
    "    labels = tokenizer(\n",
    "        examples['summary'], \n",
    "        max_length=128, \n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"✅ Data preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636db36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nandi\\AppData\\Local\\Temp\\ipykernel_11556\\2558486508.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainer initialized. Ready for fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./email_model_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized. Ready for fine-tuning!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4017b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.535211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.530083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.528388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned model saved!\n"
     ]
    }
   ],
   "source": [
    "# Start fine-tuning\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./email_model_finetuned\")\n",
    "tokenizer.save_pretrained(\"./email_model_finetuned\")\n",
    "print(\"\\nFine-tuned model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7ef11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model copied to: c:\\Users\\nandi\\OneDrive\\Desktop\\traffic\\email summ\\email_model_finetuned\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Source (where it is now)\n",
    "source = \"./email_model_finetuned\"\n",
    "\n",
    "# Destination (parent folder)\n",
    "destination = \"../email_model_finetuned\"\n",
    "\n",
    "# Copy the model folder\n",
    "if os.path.exists(source):\n",
    "    if os.path.exists(destination):\n",
    "        print(\"Destination already exists, removing old version...\")\n",
    "        shutil.rmtree(destination)\n",
    "    \n",
    "    shutil.copytree(source, destination)\n",
    "    print(f\"✅ Model copied to: {os.path.abspath(destination)}\")\n",
    "else:\n",
    "    print(\"❌ Source not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36d93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "AFTER FINE-TUNING:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary: Sarah is pleased to provide a comprehensive update on the Q1 marketing campaign project for the past two months. She has conducted extensive market analysis, competitor research, and customer surveys. She is prepared a detailed breakdown and will share in tomorrow's meeting. Sarah is confident the project will launch by the end of Q2 if they expedite the design phase.\n",
      "Summary Length: 60 words\n",
      "\n",
      "==================================================\n",
      "COMPARISON:\n",
      "==================================================\n",
      "Before: Sarah is pleased to provide a comprehensive update on the Q1 marketing campaign project for the past two months. She has conducted extensive market analysis, competitor research, and customer surveys, and has prepared a detailed breakdown to share in tomorrow's meeting. The initial budget of $50,000 may need to be increased by 15% due to rising advertising costs on social media platforms. Sarah will present findings along with recommendations next week.\n",
      "\n",
      "After:  Sarah is pleased to provide a comprehensive update on the Q1 marketing campaign project for the past two months. She has conducted extensive market analysis, competitor research, and customer surveys. She is prepared a detailed breakdown and will share in tomorrow's meeting. Sarah is confident the project will launch by the end of Q2 if they expedite the design phase.\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"./email_model_finetuned\")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(\"./email_model_finetuned\")\n",
    "\n",
    "# Create new pipeline\n",
    "finetuned_summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=finetuned_model, \n",
    "    tokenizer=finetuned_tokenizer\n",
    ")\n",
    "\n",
    "# Test on same sample email\n",
    "print(\"=\"*50)\n",
    "print(\"AFTER FINE-TUNING:\")\n",
    "print(\"=\"*50)\n",
    "summary_after = finetuned_summarizer(sample_email, max_length=100, min_length=30, do_sample=False)\n",
    "print(f\"\\nSummary: {summary_after[0]['summary_text']}\")\n",
    "print(f\"Summary Length: {len(summary_after[0]['summary_text'].split())} words\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Before: {summary_before[0]['summary_text']}\")\n",
    "print(f\"\\nAfter:  {summary_after[0]['summary_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29421f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model...\n",
      "\n",
      "ROUGE Scores:\n",
      "rouge1: 0.2105\n",
      "rouge2: 0.0556\n",
      "rougeL: 0.1579\n",
      "rougeLsum: 0.1579\n"
     ]
    }
   ],
   "source": [
    "# Load ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_samples):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for sample in test_samples:\n",
    "        email_text = sample['email']\n",
    "        true_summary = sample['summary']\n",
    "        \n",
    "        # Generate prediction\n",
    "        inputs = tokenizer(email_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(**inputs, max_length=128, min_length=30, do_sample=False)\n",
    "        pred_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(pred_summary)\n",
    "        references.append(true_summary)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "    return results\n",
    "\n",
    "# Evaluate on test set\n",
    "test_samples = eval_dataset.to_dict()\n",
    "test_samples = [{'email': test_samples['email'][i], 'summary': test_samples['summary'][i]} \n",
    "                for i in range(len(test_samples['email']))]\n",
    "\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "scores = evaluate_model(finetuned_model, finetuned_tokenizer, test_samples[:5])  # Test on 5 samples\n",
    "\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for key, value in scores.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
